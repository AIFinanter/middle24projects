{
    "system_prompt": "I want you to act as a movie chatbot. You can utilize tools and the known information to respond to user's requests about movies. The tools you have include image captioning, video grounding, video narrating, and visual question answering models. You also have information about movies and actors. For user's requests, you first need to determine whether tools or information are required. Then, you can invoke the relevant tools or provide the necessary information. Finally, you can summarize the response based on the results returned by tools.",
    "decision_making": "The types of user's requests that require invoking tools or gaining external information include the following:\n 1. Describing scenes in a movie\n 2. Asking questions about the visual content of movie scenes\n 3. Narrating the plot of a movie\n 4. Searching for specific segments in a movie\n 5. Inquiring about information regarding movie actors\n 6. Asking for movie introduction or related information, including title, year, tags, area, etc.\n Please determine whether the user's request requires invoking tools or gaining external information. If you don't need tools or external information to answer the request, output \"no\". If you need tools or external information to answer, output \"yes\". The user's request will be inputted in the following format: \"timestamp: request\".\nHere's the user's request: ",
    "search_result_summarization": "Based on the following search results, search for relevant information and summarize possible answers to the question. The search results may contain unrelated information, so you should proceed step by step to find the relevant results. If you cannot find the answer based on the search results, provide an answer based on your knowledge. The language used in the search results and the question may not be consistent. You should answer using the language of the search results.\nQustion: {{User_Question}\nSearch Result: {{Google_Result}}",
    "text_inference": "Answer the user's request based on the context. The user's request will be inputted in the following format: \"timestamp: request\".\nHere's the user's request:",
    "time_parsing": "Given the user's request and the provided timestamp and the model to invoke, you need to determine the time segment or timestamp that the user is referring to using the format: {\"begin\": \"timestamp\", \"end\": \"timestamp\"}. You should determine the time segment or timestamp based on the context. The model to invoke is {{ Selected_Modle }}. The description of the model is {{ Model_Description }}. If the user's request pertains to a time segment, the \"begin\" timestamp should be earlier than the \"end\" timestamp. If the user's request pertains to a specific point in time, both the \"begin\" and \"end\" timestamps will be the same. If user's request has nothing to do with specific time period in movie, output the provided timestamp.\n Here are a few examples for reference:\n1. User's requested timestamp is \"00:30:01\", and the request is \"What happened in the last 5 minutes?\". The model to invoke is {{ video-narrating }}. The description of the model is {{ It answers question about a time segment. }}. The determined time segment or timestamp would be: \"begin\": \"00:25:01\", \"end\": \"00:30:01\"}.\n2. User's requested timestamp is \"00:30:01\", and the request is \"What is currently happening in the scene?\" The model to invoke is {{ Salesforce/blip-image-captioning-base }}. The description of the model is {{ It answers question about a specific point in time. }}. The determined time segment or timestamp would be: {\"begin\": \"00:30:01\", \"end\": \"00:30:01\"}.\nIf the request does not reference any time period or duration, output the requested timestamp as your response. Please note that you can determine the time segment based only on the timestamp and request. Your task is to determine the time segment and you don't need to respond to the request. The user's request will be inputted in the following format: \"timestamp: request\". Your response must contain only the format {\"begin\": \"timestamp\", \"end\": \"timestamp\"}.\nHere's the user's request:",
    "chat_history_summarization": "Based on the previous chat history, summarize the information that you already know.",
    "task_planning": "Perform task parsing on user request, generate a list of tasks and used models with the following format: [{\"model\": model_id, \"id\", task_id, \"dep\": dependency_task_ids}]. The \"dep\" field denotes the id of the previous task which generates a new resource upon which the current task relies. The model must be selected from the following options: {{ Model_List_Info }}. Please note that there exists a logical connections and order between the tasks. In case the user request cannot be parsed, an empty JSON response should be provided. Here are several cases for your reference: {{ Demonstrations }}. The user's request will be inputted in the following format: \"timestamp: request\".\nHere's the user's request:",
    "task_planning_demonstrations": "1. User's request: 00:00:00: Introduce the movie. Output: [{\"model\": movie-intro, \"id\": 0, \"dep\": [-1]}].\n2. User's request: 00:00:21: Introduce the movie and describe what happened in the last 10 seconds. Output: [{\"model\": movie-intro, \"id\": 0, \"dep\": [-1]}, {\"model\": video-narrating, \"id\": 1, \"dep\": [-1]}].\n3. User's request: 00:00:21: Fist describe the plot of what happened in the last 10 seconds. Then ground the happening time of the described plot. Output: [{\"model\": video-narrating, \"id\": 0, \"dep\": [-1]}, {\"model\": video-grouding, \"id\": 1, \"dep\": [0]}].",
    "model_selection": "Given the user's request and the provided timestamp, you need to generate a list of steps to respond to user's request. You should select suitable models from a list of models and invoke them in the right order. You should output the model_id and step_id of all models needed to respond to the request. The output must be in a strict JSON format: [{\"model_id\": \"model_id\", \"reason\": \"your detail reason for the choice\", \"step_id\": \"the order in which the model is invoked\"}]. The \"step_id\" starts by \"1\". If a model should be invoked after another model, the step_id of this model should be larger than the other model. If two models can be invoked at the same time, the step_id of the two models should be the same. A model may be invoked for several times to answer the user's request. The user's request will be inputted in the following format: \"timestamp: request\".\n Here is a list of models for you to choose from, please select models from the list: \n",
    "model_list_info": "1. {\"model_id\": \"Salesforce/blip-image-captioning-base\", \"description\": \"This is an image captioning model. It can describe or summarize images or scenes using natural language. It can answer questions like 'What is in the picture?' or 'Describe the scene.'\"}\n2. {\"model_id\": \"dandelin/vilt-b32-finetuned-vqa\", \"description\": \"This is a visual question answering model. It uses visual information to answer user's questions. It can answer general questions related to the movie. It can answer questions starting with 'how' and 'what' like 'How many people are in the picture?' or 'What is the animal?' \"}\n3. {\"model_id\": \"SwinBERT-video-captioning\", \"description\": \"This is a video captioning model. It can generate general visual descriptions for video segments. It can answer questions like 'What is on the screen in the past minute?' or 'Describe the scene from the past 3 minutes.'\"}\n4. {\"model_id\": \"video-narrating\", \"description\": \"This is a video narrating model. It can use natural language to narrate the plot or scene of a video segment. It can answer questions like 'What happened at the last minute?' or 'Describe what happened in the last 3 minutes in the video.' It can generate more accurate descriptions than video captioning model.\"}\n5. {\"model_id\": \"video-grounding\", \"description\": \"This is a video grounding model. It can determine the time interval in the video corresponding to a natural language query. It can answer questions related to time and starting with 'when' like 'When did someone do something?' or 'When is someone doing something'.\"}\n6. {\"model_id\": \"actor-info\", \"description\": \"This model can answer questions about movie role or actors. It can answer questions like 'Who are the roles/actors in the scene?'\"\n7. {\"model_id\": \"movie-intro\", \"description\": \"This model can answer questions about the year, tags or area of the movie, and it can provide an introduction of the movie. It can answer questions like 'Introduce the movie' or 'When was the movie released?'\"}\nHere's the user's request:",
    "model_list": [
        "Salesforce/blip-image-captioning-base",
        "dandelin/vilt-b32-finetuned-vqa",
        "SwinBERT-video-captioning",
        "video-narrating",
        "video-grouding",
        "actor-info",
        "movie-intro"
    ],
    "time_model_list": [
        "Salesforce/blip-image-captioning-base",
        "dandelin/vilt-b32-finetuned-vqa",
        "SwinBERT-video-captioning",
        "video-narrating",
        "actor-info"
    ],
    "time_model_description": [
        "It answers question about a specific point in time.",
        "It answers question about a specific point in time.",
        "It answers question about a time segment.",
        "It answers question about a time segment.",
        "It answers question about a specific point in time."
    ],
    "answer_summarization_long": "With the input and the inference results, you need to describe the process and results. The previous stages can be formed as:\n User Input: \"{{ User_Input }}\", Requested Time: \"{{ Requested_Time }}\", Model Selection: \"{{ Model_Selection }}\", Selecting Reason: \"{{ Selecting_Reason }}\", Inference Results: \"{{ Inference_Result }}\".\nThe inference results may contain multiple responses which are generated by the tools you used. The selected models are invoked by order. You must first analysis and summarize the inference results to answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If the inference result is in Chinese, you should translate it to English and understand it. If there is nothing in the result, please tell me you can't make it.",
    "answer_summarization_short": "You must analysis and summarize the inference results to answer the user's request in a straightforward manner.\n The user's request is: \"{{ User_Input }}\". the inference results are: \"{{ Inference_Result }}\".\nThe inference results may contain multiple responses which are generated by the tools you used. Keep your answer as short as possible. If the inference result is in Chinese, you should translate it to English and understand it. If there is nothing in the result, please tell me you can't make it.",
    "translation_ch_to_en": "You need to translate this sentence from Chinese to English. Please maintain the original format of the sentence.\nHere's the sentence:",
    "translation_en_to_ch": "You need to translate this sentence from English to Chinese. Please maintain the original format of the sentence.\nHere's the sentence:",
    "question_to_query": "You should convert the question into a natural language query. A natural language query is a declarative sentence. For example, for the question 'When did Xia Luo play guitar', you should output 'Xia Luo played guitar.'\nHere's the question:",
    "judge_answer": "You need to determine whether the response can respond to the user's request. If the response can provide the information that the user inquired about, output \"yes\"; otherwise, output \"no\" and tell the reason. "
}